<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="Long Zeng">
    <meta name="author" content="Long Zeng">
    <link rel="icon" href="./bootstrap/immv-logo.ico">

    <title>Dr. Zeng's homepage</title>

    <!-- Bootstrap core CSS, href gives URL of stylesheet; rel describes the relationship between this doc and the URL doc. -->
    <link href="./bootstrap/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <link href="./bootstrap/assets/css/ie10-viewport-bug-workaround.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="homepage.css" rel="stylesheet">

    <!-- Just for debugging purposes. Don't actually copy these 2 lines! -->
    <!--[if lt IE 9]><script src="../../assets/js/ie8-responsive-file-warning.js"></script><![endif]-->
    <script src="./bootstrap/assets/js/ie-emulation-modes-warning.js"></script>

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  </head>

  <body>

    <nav class="navbar navbar-inverse navbar-fixed-top">
      <div class="container">
        <div id="navbar" class="collapse navbar-collapse">
          <ul class="nav navbar-nav">
            <li><a href="#about">About Me</a></li>
            <li><a href="#news">News</a></li> 
            <li><a href="#pub">Publications</a></li>
            <li><a href="#teach">Teaching</a></li> 
            <li><a href="#projects">Projects</a></li>           
            <li><a href="#students">Students</a></li>
            <!--<li><a href="#pub">Services</a></li>  -->                  
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </nav>

    <div id="about" class="container">
        <h1>Long Zeng  曾龙</h1>
        <h4>Intelligent Manufacutring and Machine Vision Group</h4>
    </div>
    <hr>
    
    <div class="container">
      <div class="row">
        <div class="col-xs-2">
          <!--<img src="me.jpg" class="img-responsive" alt="Responsive image">-->
          <img src="zenglong.jpg" class="img-thumbnail" alt="Responsive image">
        </div>

        <div class="col-xs-3">
          <h4>Associate Professor, Ph.D.</h4>
          <address>
            <a href="https://www.sigs.tsinghua.edu.cn/">Tsinghua University</a>, <a href="https://www.sigs.tsinghua.edu.cn/">SIGS</a>, Mphil/Ph.D. Supervisor<br>
            <strong>--Director</strong>, Joint Research Center of Intelligent Service Robotic Technology;<br> 
            <strong>--Co-founder & Chief Scientist</strong>, <a href="http://www.gzfwzn.com/">Fuwei Intelligent Tech. Co. Ltd</a> (start-up);<br>
            <strong>--Technical advisor</strong>, <a href="https://www.pudutech.com/zh-CN">Pudu Tech. Co. Ltd</a> (Unicorn);
          </address><address>
            <strong>Email: <a href="zenglong@sz.tsinghua.edu.cn">zenglong@sz.tsinghua.edu.cn</a></strong><br>
            <strong><a href="https://www.sigs.tsinghua.edu.cn/cl/main.htm">[中文主页]</a>
              <a href="https://scholar.google.com/citations?user=72QbaQwAAAAJ&hl=en&oi=sra">[Google Scholar] </a>
              <a href="https://github.com/jackyzengl">[Github]</a></strong><br>
          </address>
        </div>

        <div class="col-xs-7" align="justify">
          <p>I am an tenure-tracked PI at the Information and Data Institute, SIGs, Tsinghua. I obtained my Ph.D. degree from Hong Kong University of Sci. and Tech..
            My research interest is Industrial Embodied Intelligence, to solve challenging problems in both product design and robotic manufacturing scenarios. 
            We study new deep learning methods exploring the unique characteristics of industrial data and mainly apply in <strong>Embodied Intelligent Industrial Robotics</strong> (EIIR, e.g.Flexible Assembly), 
            <strong>Embodied Intelligent Service Robotics</strong> (EISR), <strong>Intelligent CAD</strong> (e.g. Sketch-based Engineering Product Modeling). </p>
          <p>Our academic results are partially industrialized and verified by industrial products. I am the <strong>Co-founder & Chief Scientist</strong> 
            in <a href="http://www.gzfwzn.com/">Fuwei Intelligent Tech. Co. Ltd</a> (EIIR start-up company), the <strong>Technical advisor</strong>  in <a href="https://www.pudutech.com/zh-CN">Shenzhen Pudu Tech. Co. Ltd</a> (a <strong>Unicorn</strong> EISR company)
          </p> 
          <p>
            <strong>For Prospective Students</strong>: I am actively looking for highly self-motivated <strong><u>Ph.D. students, Post-doctoral, and Research Assistants</u></strong>(undergraduate/graduate level). Please email me with your CV and transcripts if you are interested in our research.
          </p>          
        </div>
      </div>

      <hr>
      <div id="news" class="container">
        <h3>News</h3>
        <div><strong>[June, 2025]</strong> One paper was accepted by Journal of Intelligent Manufacturing <strong>JIM 2025</strong>. Congratualations to Zhaobo Xu</a></div>
        <div><strong>[June, 2025]</strong> One paper was accepted by <strong>ACM Siggraph Asia 2025, Hong Kong</strong> and simultaneously accepted to <strong>ACM Transaction on Graphics</strong>. Congratualations to <a href="https://ydove0324.github.io/Imaginarium/"> Xiaoming Zhu</a></div>
        <div><strong>[June, 2025]</strong> One paper was accepted by conf. <strong>ICCV 2025</strong>. Congratualations to <a href="https://cstnetwork.github.io/"> Xi Cheng</a></div>
        <div><strong>[June, 2025]</strong> One paper was accepted by conf. <strong>IROS 2025</strong>. Congratualations to <a href="https://github.com/jackyzengl/Diffusion_Suction"> DingTao Huang</a></div>
        <div><strong>[May, 2025]</strong> Won <strong>ICRA 2025 Outstanding Reviewer Award</strong> (only 5, selected from more than 7.000 reviewers). </div>
        <div><strong>[April, 2025]</strong> One paper was accepted by conf. <strong>RSS 2025</strong>. Congratualations to Zheng Zijie and Li Zeshun</div>
        <div><strong>[April, 2025]</strong> One paper was accepted by conf. <strong>ICRA 2025</strong>. Congratualations to Xiang Haodong </div>
        <div><strong>[Jan., 2025]</strong> Joined tenured-tracked PI, SIGS, Tsinghua University. <strong>New journey!</strong></div>
        <div><strong>[Oct., 2024]</strong> Chair three session in IEEE IROS 2024 in Abu Dhabi!</strong></div>
        <div><strong>[Aug., 2024]</strong> One paper was accpeted by Journal of Manufacturing System (<strong>IF 12.2</strong>)! Congratualations to Xu Zhaobo, Zhang chaoran, and Hu song.</div>  
        <div><strong>[Aug., 2024]</strong> four papers were accpeted by IEEE IROS 2024! Congratualations to <a href="https://jackyzengl.github.io/GRID.github.io/">Ni zhe</a>, Xie Yihan, Jing Xinghui, and <a href="https://github.com/dingthuang/SD-Net">Huang Dingtao</a></div>               
        <div><strong>[May., 2024]</strong> Chair two session in IEEE ICRA 2024 in Yokohama Japan!</strong></div>
        <div><strong>[Nov., 2014]</strong> Joined Tsinghua Shenzhen Graduate School, Tsinghua University.</div>
      </div>

      <hr>
      <h3 id="pub" class="container">
          <h3>Representative Publications</h3>
          <div> <a href="https://scholar.google.com/citations?user=72QbaQwAAAAJ&hl=en&oi=sra">Full paper list at Google Scholar</a>. <br>
          *,# indicates  corresponding and equal contribution author</div>
          <hr>
         <h3><strong>2025</strong></h3>    
                  
         <div class="row">
          <div class="col-xs-3">
            <img src="./images/ATP-KG25JIM.png" class="img-responsive" alt="Responsive image"><br><br> 
          </div>
          <div class="col-xs-8" align="justify">  
            <div><strong>Assembly task planning framework based on knowledge graph</strong></div>
            <div><p>Zhaobo Xu#, Chaoran Zhanga#, Sheng Houa#, Zhaochun Han, <strong>Long Zeng*</strong>, Pingfa Feng*.</p></div>
            <div><p>Journal of Intelligent Manufacturing (<strong>JIM</strong>), 2025.</p></div>
            <div><p><a href="https://link.springer.com/article/10.1007/s10845-025-02695-1">[ Paper ] </a>
                <a href="https://github.com/jackyzengl/Diffusion_Suction">[ Code ]</a>
              </p>                
            </div>
          </div>
        </div>   

         <div class="row">
          <div class="col-xs-3">
            <img src="./images/Imaginarium25SiggraphAsia.png" class="img-responsive" alt="Responsive image"><br><br> 
          </div>
          <div class="col-xs-8" align="justify">  
            <div><strong>Imaginarium: Vision-guided High-Quality 3D Scene Layout Design</strong></div>
            <div><p>XiaoMing Zhu#, Xu Huang#, QingHongBing Xie, Zhi Deng*, JunSheng Yu, YiRui Guan, ZhongYuan Liu, Lin Zhu, QiJun Zhao, LiGang Liu, <strong>Long Zeng*</strong>.</p></div>
            <div><p>ACM SIGGRAPH Asia 2025 and ACM Trans. Graph., Vol. 44, No. 6, (<strong>Acceptance rate: 13.58%</strong>), 2025.</p></div>
            <div><p><a href="https://arxiv.org/abs/2510.15564">[ arXiv Paper ] </a>
                <a href="https://ydove0324.github.io/Imaginarium/">[ Project page with dataset & Code ]</a>
              </p>                
            </div>
          </div>
        </div>             
         
         <div class="row">
          <div class="col-xs-3">
            <img src="./images/DiffusionSuction25IROS.png" class="img-responsive" alt="Responsive image"><br><br> 
          </div>
          <div class="col-xs-8" align="justify">  
            <div><strong>Diffusion Suction Grasping with Large-Scale Parcel Dataset</strong></div>
            <div><p>Ding-Tao Huang, Debei Hua, Dongfang Yu, Xinyi He, En-Te Lin, Liang-hong Wang, Jin-liang Hou, <strong>Long Zeng*</strong>.</p></div>
            <div><p>2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (<strong>IROS</strong>), 2025.</p></div>
            <div><p><a href="https://arxiv.org/abs/2502.07238">[ arXiv Paper ] </a>
                <a href="https://github.com/jackyzengl/Diffusion_Suction">[ Code ]</a>
              </p>                
            </div>
          </div>
        </div>    

         <div class="row">
          <div class="col-xs-3">
            <img src="./images/EIIR25JMS.png" class="img-responsive" alt="Responsive image"><br><br> 
          </div>
          <div class="col-xs-8" align="justify">  
            <div><strong>Embodied Intelligent Industrial Robotics: Concepts and Techniques</strong></div>
            <div><p>Chaoran Zhang#, Chenhao Zhang#, Zhaobo Xu, Qinghongbing Xie, Pingfa Feng, <strong>Long Zeng*</strong>.</p></div>
            <div><p>Submitted to International Journal of Manufacturing System (<strong>JMS, IF12.2</strong>), 2025.</p></div>
            <div><p><a href="https://arxiv.org/pdf/2505.09305">[ arXiv Paper ] </a>
                <a href="https://github.com/jackyzengl/EIIR">[Project page]</a>
              </p>                
            </div>
          </div>
        </div>

         <div class="row">
          <div class="col-xs-3">
            <img src="./images/HumanAesExpert25arXiv.png" class="img-responsive" alt="Responsive image"><br><br> 
          </div>
          <div class="col-xs-8" align="justify">  
            <div><strong>HumanAesExpert: Advancing a Multi-Modality Foundation Model for Human Image Aesthetic Assessment</strong></div>
            <div><p>Zhichao Liao#, Xiaokun Liu, Wenyu Qin, Dezhi Zheng, Peixin Xie, Qingyu Li, Qiulin Wang, Pengfei Wan, Di Zhang, <strong>Long Zeng*</strong>, Pingfa Feng.</p></div>
            <div><p>arXiv preprint:2503.23907 (<strong>arXiv</strong>), 2025.</p></div>
            <div><p><a href="https://arxiv.org/abs/2503.23907">[ arXiv Paper ] </a>
                <a href="https://github.com/KwaiVGI/HumanAesExpert">[ Code & Dataset ] </a>   
                <a href="https://humanaesexpert.github.io/HumanAesExpert/">[Project page]</a>
              </p>                
            </div>
          </div>
        </div>
         
        <div class="row">
          <div class="col-xs-3">
            <img src="./images/SPF-Portrait25arXiv.png" class="img-responsive" alt="Responsive image"><br><br> 
          </div>
          <div class="col-xs-8" align="justify">  
            <div><strong>SPF-Portrait: Towards Pure Portrait Customization with Semantic Pollution-Free Fine-tuning </strong></div>
            <div><p>Zhichao Liao#, Xiaole Xian#, Qingyu Li, Wenyu Qin, Pengfei Wan, Weicheng Xie, <strong>Long Zeng*</strong>, Linlin Shen, Pingfa Feng.</p></div>
            <div><p>arXiv preprint:2504.00396 (<strong>arXiv</strong>), 2025.</p></div>
            <div><p><a href="https://arxiv.org/abs/2504.00396">[ arXiv Paper ] </a>
                <a href="https://github.com/KwaiVGI/SPF-Portrait">[ Code & Dataset ] </a>   
                <a href="https://spf-portrait.github.io/SPF-Portrait/">[Project page]</a>
              </p>                
            </div>
          </div>
        </div>

         <div class="row">
          <div class="col-xs-3">
            <img src="./images/THUD++25IJRR.png" class="img-responsive" alt="Responsive image"><br><br> 
          </div>
          <div class="col-xs-8" align="justify">  
            <div><strong>THUD++: Large-Scale Dynamic Indoor Scene Dataset and Benchmark for Mobile Robots</strong></div>
            <div><p>Zeshun Li#, Fuhao Li#, Wanting Zhang#, Zijie Zheng, Xueping Liu, Tao Zhang, Yongjin Liu, <strong>Long Zeng*</strong>.</p></div>
            <div><p>arXiv print, 2025.</p></div>
            <div><p><a href="https://arxiv.org/abs/2412.08096">[ arXiv Paper ] </a>
                <a href="https://github.com/jackyzengl/THUD-plus-plus?tab=readme-ov-file">[ Code & Dataset ] </a>
                <a href="https://jackyzengl.github.io/THUD-plus-plus.github.io/">[Project page]</a>
              </p>                
            </div>
          </div>
        </div>

         <div class="row">
          <div class="col-xs-3">
            <img src="./images/CstNet25ICCV.png" class="img-responsive" alt="Responsive image"><br><br> 
          </div>
          <div class="col-xs-8" align="justify">  
            <div><strong>Constraint-Aware Feature Learning for Parametric Point Cloud</strong></div>
            <div><p>Xi Cheng, Ruiqi Lei, Di Huang, Zhichao Liao, Fengyuan Piao, Yan Chen, Pingfa Feng, <strong>Long Zeng*</strong>.</p></div>
            <div><p>2025 International Conference on Computer Vision (<strong>ICCV, 2025</strong>).</p></div>
            <div><p><a href="https://arxiv.org/abs/2411.07747">[ arXiv Paper ] </a>
                <a href="https://github.com/cstnetwork/cstnet">[ Code ] </a>
                <a href="https://cstnetwork.github.io/">[Project page]</a>
              </p>                
            </div>
          </div>
        </div>

         <div class="row">
          <div class="col-xs-3">
            <img src="./images/2DGS-Room25IROS.png" class="img-responsive" alt="Responsive image"><br><br> 
          </div>
          <div class="col-xs-8" align="justify">  
            <div><strong>2DGS-Room: Seed-Guided 2D Gaussian Splatting with Geometric Constrains for High-Fidelity Indoor Scene Reconstruction</strong></div>
            <p>Wanting Zhang#, Haodong Xiang, Zhichao Liao, Xiansong Lai, Xinghui Li*, <strong>Long Zeng*</strong>.</p>
            <p>arXiv print, 2025.</p>
            <p><a href="https://arxiv.org/abs/2412.03428">[ arXiv Paper ]</a>
                <a href="https://github.com/jackyzengl/2DGS-Room">[ Code ] </a>
            </p>
          </div>
        </div>

        <div class="row">
          <div class="col-xs-3">
            <img src="./images/DVS25RSS.png" class="img-responsive" alt="Responsive image"><br><br> 
          </div>
          <div class="col-xs-8" align="justify">  
            <div><strong>Demostrating DVS: Dynamic Virtual-Real Simulation Platform for Mobile Robotic Tasks</strong></div>
            <div><p>Zijie Zheng#, Zeshun Li#, Yunpeng Wang#, Qinghongbing Xie, <strong>Long Zeng*</strong>.</p></div>
            <div><p>2025 Robotics: Science and Systems (<strong>RSS</strong>), 2025.</p></div>
            <div><p><a href="./papers/1-DVS25RSS.pdf">[ Pre-print Paper ]</a>
                   <a href="./papers/1-DVS25RSS.mp4">[ Video ] </a>
                  </p>                
            </div>
          </div>
        </div>

         <div class="row">
          <div class="col-xs-3">
            <img src="./images/GaussianRoom25ICRA.png" class="img-responsive" alt="Responsive image"><br><br> 
          </div>
          <div class="col-xs-8" align="justify">  
            <div><strong>GaussianRoom: Improving 3D Gaussian Splatting with SDF Guidance and Monocular Cues for Indoor Scene Reconstruction</strong></div>
            <p>Haodong Xiang#, Xinghui Li#, Kai Cheng#, Xiansong Lai, Wanting Zhang, Zhichao Liao, <strong>Long Zeng*</strong>, Xueping Liu*</strong>.</p>
            <p>IEEE Conference Robotic and Automation (<strong>ICRA</strong>), 2025.</p>
            <p><a href="https://arxiv.org/abs/2405.19671">[ arXiv Paper ]</a>
            <a href="https://github.com/jackyzengl/GaussianRoom">[ Code ] </a>
          </p>
          </div>
        </div>

         <h3><strong>2024</strong></h3>

         <div class="row">
          <div class="col-xs-3">
            <img src="./images/SketchGen24MM.png" class="img-responsive" alt="Responsive image"><br><br> 
          </div>
          <div class="col-xs-8" align="justify">  
            <div><strong>Freehand Sketch Generation from Mechanical Components</strong></div>
            <div><p><a href="https://lzc-sg.github.io/">Zhichao Liao</a>, Di Huang, Heming Fang, Yue Ma, Fengyuan Piao, Xinghui Li, <strong>Long Zeng*</strong>, Pingfa Feng.</p></div>
            <div><P>The 32th ACM International Conference on Multimedia (<strong>ACM MM</strong>), 2024.</div>
            <div><p><a href="https://arxiv.org/abs/2408.05966">[ Paper ]</a>
                <a href="https://github.com/di-huang/Freehand-Sketch-Generation-from-Mechanical-Components/">[Code]</a>
                <a href="https://mcfreeskegen.github.io/">[Project page]</a></p>
            </div>
          </div>
        </div>

          <div class="row">
            <div class="col-xs-3">
              <img src="./images/ZengL24JMS.png" class="img-responsive" alt="Responsive image"><br><br> 
            </div>
            <div class="col-xs-8" align="justify">  
              <div><strong>Reconfigurable Flexible Assembly Model and Its Implementation for Cross-Category Products</strong></div>
              <div><p>Zhaobo Xu#, Chaoran Zhang#, Song Hu#, Pingfa Feng, <strong>Long Zeng*</strong>.</p></div>
              <div><p>Journal of Manufacturing Systems (<strong>JMS, IF 12.2</strong>), 2024.</p></div>
              <div><p><a href="https://www.sciencedirect.com/science/article/pii/S0278612524001857">[ Paper ]</a></p>
              </div>
            </div>
          </div>

          <div class="row">
            <div class="col-xs-3">
              <img src="./images/DADA.png" class="img-responsive" alt="Responsive image"><br><br> 
            </div>
            <div class="col-xs-8" align="justify">  
              <div><strong>Dual-Alignment Domain Adaptation for Pedestrian Trajectory Prediction</strong></div>
              <div><p>Wenzhan Li#, Fuhao Li#, Xinghui Jing, Pingfa Feng, <strong>Long Zeng*</strong>.</p></div>
              <div><p>J IEEE Robotics and Automation Letters(<strong>RAL,  Volume: 9, Issue: 12</strong>), 2024.</p></div>
              <div><p><a href="https://ieeexplore.ieee.org/document/10719677">[ Paper ]</a>
                <a href="https://github.com/jackyzengl/DADA">[ Code ]</a>&nbsp; </p> 
              </div>
            </div>
          </div>

          <div class="row">
            <div class="col-xs-3">
              <img src="./images/THUD24ICRA.png" class="img-responsive" alt="Responsive image"><br><br> 
            </div>
            <div class="col-xs-8" align="justify">
              <div><strong>Mobile Oriented Large-Scale Indoor Dataset for Dynamic Scene Understanding</strong></div>
              <div><p>Yi-Fan Tang#, Cong Tai#, Fang-Xin Chen#, Wanting Zhang, Tao Zhang, Yongjin Liu, <strong>Long Zeng*</strong>.</p></div>
              <div><P>IEEE Conference Robotic and Automation (<strong>ICRA</strong>), 2024.</p></div>
              <div><P><a href="https://arxiv.org/abs/2406.19791">[ Paper ]</a>&nbsp;         
              <a href="https://jackyzengl.github.io/THUD-Robotic-Dataset.github.io/">[ Project Page ]</a>&nbsp;</p>
              </div>
            </div>
          </div>

          <div class="row">
            <div class="col-xs-3">
              <img src="./images/SDNet24IROS.png" class="img-responsive" alt="Responsive image"><br><br> 
            </div>
            <div class="col-xs-8" align="justify">
              <div><strong>SD-Net: Symmetric-Aware Keypoint Prediction and Domain Adaptation for 6D Pose Estimation In Bin-picking Scenarios</strong></div>
              <div><p>Ding-Tao Huang, En-Te Lin, Lipeng Chen2, Li-Fu Liu1, <strong>Long Zeng*</strong>.</p></div>
              <div><p>IEEE/RSJ International Conference on Intelligent Robots and Systems(<strong>IROS</strong>), 2024.</p></div>
              <div><p><a href="https://arxiv.org/abs/2403.09317">[ Paper ]</a>&nbsp;  
              <a href="https://github.com/dingthuang/SD-Net?tab=readme-ov-file">[ Code & Dataset ]</a>&nbsp;              
              </p></div>
            </div>
          </div>
        
          <div class="row">
            <div class="col-xs-3">
              <img src="./images/GRID24IROS.png" class="img-responsive" alt="Responsive image"><br><br> 
            </div>
            <div class="col-xs-8" align="justify">
              <div><strong>GRID:Scene-Graph-based Instruction-driven Robotic Task Planning</strong></div>
              <div><p>Zhe Ni#, Xiaoxin Deng#, Cong Tai#, Xinyue Zhu, Qinghongbing Xie, Weihang Huang, Xiang Wu, <strong>Long Zeng*</strong>.</p></div>
              <div><p>IEEE/RSJ International Conference on Intelligent Robots and Systems(<strong>IROS</strong>), 2024.</p></div>
              <div><p><a href="https://arxiv.org/abs/2406.19791">[ Paper ]</a>&nbsp;         
              <a href="https://jackyzengl.github.io/GRID.github.io/">[ Project Page ]</a>&nbsp;
              <a href="https://github.com/jackyzengl/GRID">[ Code & Dataset ]</a>&nbsp;              
              </p></div>
            </div>
          </div>
        
          <div class="row">
            <div class="col-xs-3">
              <img src="./images/ParametricPlusPlus24IROS.png" class="img-responsive" alt="Responsive image"><br><br> 
            </div>
            <div class="col-xs-8" align="justify">
              <div><strong>ParametricNet++: A 6DoF Pose Estimation Network with Sparse Keypoint Recovery for Parametric Shapes in Stacked Scenarios</strong></div>
              <div><p>Yi Han Xie, Wei Jie Lv, Xin Yu Zhang, Yi Hong Chen, <strong>Long Zeng*</strong>.</p></div>
              <div><p>IEEE/RSJ International Conference on Intelligent Robots and Systems(<strong>IROS</strong>), 2024.</p></div>
              <div><p><a href="https://ieeexplore.ieee.org/document/10802005/">[ Paper ]</a>&nbsp;
              <a href="https://github.com/lvwj19/ParametricNet">[ Code & Dataset]</a>(Link to ParametricNet)&nbsp;              
              </p></div>
            </div>
          </div>
          
          <h3><strong>Before 2024</strong></h3>

          <div class="row">
            <div class="col-xs-3">
              <img src="./images/YOLOV4-SA22AMT.png" class="img-responsive" alt="Responsive image"><br><br> 
            </div>
            <div class="col-xs-8" align="justify">
              <div><strong>Attention-based deep learning for chip-surface-defect detection</strong></div>
              <div><p> Shuo Wang#, Hong Yu Wang, Fan Yang, Fei Liu, <strong>Long Zeng*</strong>.</p></div>
              <div><p>The International Journal of Advanced Manufacturing Technology (<strong>IJAMT</strong>), pp.1957–1971, 2022.</p></div>
              <div><p><a href="https://link.springer.com/article/10.1007/s00170-022-09425-4">[ Paper ]</a>&nbsp; 
              <a href="https://pan.baidu.com/s/1DsZyyO4ITtsLWqFyGS2KEA"> [ Dataset] </a> (<strong>Baidu code</strong>: gcve; <strong>Unzip code</strong>:ws980401) &nbsp;              
            </p></div>
            </div>
          </div>
          
          <div class="row">
            <div class="col-xs-3">
              <img src="./images/ParametricNet21ICRA.png" class="img-responsive" alt="Responsive image"><br><br> 
            </div>
            <div class="col-xs-8" align="justify">
              <div><strong>ParametricNet: 6DoF Pose Estimation Network for Parametric Shapes in Stacked Scenarios</strong></div>
              <div><p><strong>Long Zeng#</strong>, W. J. Lv, X. Y. Zhang, Y. J. Liu*.</p></div>
              <div><p>IEEE Conference Robotic and Automation (<strong>ICRA</strong>), 2021.</p></div>
              <div><p><a href="https://ieeexplore.ieee.org/document/9561181">[ Paper ]</a>&nbsp; 
              <a href="https://github.com/lvwj19/ParametricNet"> [ Code & Dataset ] </a>&nbsp;              
            </p></div>
            </div>
          </div>

          <div class="row">
            <div class="col-xs-3">
              <img src="./images/pprnet++21TASE.png" class="img-responsive" alt="Responsive image"><br><br> 
            </div>
            <div class="col-xs-8" align="justify">
              <div><strong>PPR-Net++: Accurate 6D pose estimation in stacked scenarios</strong></div>
              <div><p><strong>Long Zeng#</strong>, W. J. Lv, Z. K. Dong, Y. J. Liu*.</p></div>
              <div><p>IEEE Transaction on Automation and Science (<strong>JCR Q1</strong>), 2021, pp.1-13.</p></div>
              <div><p><a href="https://ieeexplore.ieee.org/document/9537584">[ Paper ]</a>&nbsp; 
              <a href="https://github.com/lvwj19/PPR-Net-plus"> [ Code & Dataset ] </a>&nbsp;              
            </p></div>
            </div>
          </div>

          <div class="row">
            <div class="col-xs-3">
              <img src="./images/pprnet19IROS.png" class="img-responsive" alt="Responsive image"><br><br> 
            </div>
            <div class="col-xs-8" align="justify">
              <div><strong>PPR-Net: Point-wise Pose Regression Network for Instance Segmentation and 6D Pose Estimation in Bin-picking Scenarios</strong></div>
              <div><p>Z. K. Dong#, S. C. Liu#, T. Zhou#, H. Cheng, <strong>Long Zeng*</strong>, X. Y. Yu, H. D. Liu.</p></div>
              <div><p>IEEE/RSJ International Conference on Intelligent Robots and Systems, Macau, China, pp. 1773-1780(<strong>IROS</strong>), 2019.</p></div>              
              <div><p><a href="https://ieeexplore.ieee.org/document/8967895">[ Paper ]</a>&nbsp; 
              <a href="https://github.com/lvwj19/PPR-Net-plus"> [ Code & Dataset] (Can be found in the paper PPR-Net++)</a>&nbsp;              </p>
              </div>
            </div>
          </div>

          <div class="row">
            <div class="col-xs-3">
              <img src="./images/SpiderCNN18ECCV.png" class="img-responsive" alt="Responsive image"><br><br> 
            </div>
            <div class="col-xs-8" align="justify">
              <div><strong>SpiderCNN: Deep Learning on Point Sets with Parameterized Convolutional Filters</strong></div>
              <div>Yifan Xu# (My student), Tianqi Fan, Mengye Xu, <strong>Long Zeng</strong>, Yu Qiao*.</div>
              <div>European Conferenceon Computer Vision (<strong>ECCV</strong>), 2018.</div>
              <div><strong>Contribution:</strong>It is challenging to apply CNNs to domains lacking a regular underlying structures such as 3D point clouds. Thus, we propose a novel convolutional architecture, 
                termed SpiderCNN. A new paramerized convolutional operations is designed which can extend CNN from regular grids to irregular point sets. 
                SOTA performance on ModelNet40, i.e. 92:4%, was achieved (<strong>Google scholar citation >1000.</strong>).</div>
              <div><a href="https://link.springer.com/chapter/10.1007/978-3-030-01237-3_6">[ Paper ]</a>&nbsp; 
              <a href="https://github.com/xyf513/SpiderCNN">[ Code & Dataset ]</a>&nbsp;              
              </div>
            </div>
          </div>
      <hr>
      <div id="projects" class="container" align="justify">
        <h3>Projects</h3>
        <div>1. 2022/11-2025/10, Hand drawn sketch collection and generation algorithm, National Key Research and Development Program "Industrial Software" Key Special Project (Project No. 2022YFB3303101), PI.</div>
        <div>2. 2020/01-2023/12, National Natural Science Foundation of China (General Program # 61972220), Sketch-based parametric modeling with large data,<strong>PI</strong>.</div>
        <div>3. 2016/01-2018/12, National Natural Science Foundation of China (Youth Program #61502263), Sketch-based assembly modeling with Intelligent Feature, <strong>PI</strong>.</div>
        <div>4. 2022/01-2024/12, Research on 6D pose estimation method for parameterized parts in industrial stacking scenarios, Guangdong Natural Science General Project (No. 2022A1515011234), <strong>PI</strong>.</div>
        <div>5. 2021/11-2024/11, Tsinghua-Purdue Intelligent Service Robot Technology Joint Research Center, a college level institution, 11.0M, <strong>Director</strong>.</div>        
      </div>
      <hr>

      <div id="teach" class="container">
        <h3>Courses</h3>
        <div class="row">
          <div class="col-xs-3">
            <img src="./images/ML-Course.png" class="img-responsive" alt="Responsive image"><br><br> 
          </div>
          <div class="col-xs-8" align="justify">
            <div><strong>《机器学习实践与应用》(Machine Learning: Practices and Applications)</strong></div>
            <div>Lectured in every fall semester for graduate students, starting from 2021.</div>
            <div>It covers both machine learning methods (such as Linear Model, SVM, Bayesian classifier, Decision tree, and Cluster), 
              deep learning methods (such as AlexNet, ResNet et al.), and reinforcement learning. They are all organized by the same machine learning framework, 
              i.e. models, loss functions, and learning algorithms. </div>
          </div>
        </div>

        <div class="row">
          <div class="col-xs-3">
            <img src="./images/CAD-Course.png" class="img-responsive" alt="Responsive image"><br><br> 
          </div>
          <div class="col-xs-8" align="justify">
            <div><strong>《现代CAD方法与技术》(Modern CAD Methodology and Technology)</strong></div>
            <div>Lectured in every spring semester for graduate students, starting from 2017.</div>
            <div>It covers both traditional CAD methods (such as Bezier, B-spline curves and surface) and AI-based intelligent CAD methods.
              Based on the basic process of hand drawn engineering product modeling, this course is divided into several research topics. 
              In combination with the course project, the theory and classical algorithms of each topic are explained in simple terms, 
              including the pretreatment of hand drawn sketch, sketch scene understanding, basic theory of B-spline curve and surface, parametric model representation, 
              hand drawn model acquisition and other sketch modeling technologies, deep learning and its application in hand drawn modeling. </div>
          </div>
        </div>

        <div class="row">
          <div class="col-xs-3">
            <img src="./images/Product-Course.png" class="img-responsive" alt="Responsive image"><br><br> 
          </div>
          <div class="col-xs-8" align="justify">
            <div><strong>《产品设计与开发》(Product Design and Development)</strong></div>
            <div>Lectured in every fall semester for graduate students, starting from 2016.</div>
            <div>This course divides the whole process of product design and development into four stages: requirement analysis, conceptual design, detailed design and product exhibition. 
              The knowledge and methods used in each stage are organized and explained structurally, which is easy for students to master and apply. 
              The first three stages need to submit a product design progress report for each. In the stage of product exhibition, the course organizes a product design conference and 
              invites business people to act as judges. Excellent works have the opportunity to transform their achievements directly.
              The course emphasizes the organic combination of practice and theory. Students first form a project team and are equipped with project instructors. 
              Students apply the knowledge and methods learned in class directly to the curriculum project. In the process of project implementation, 
              students will exercise team spirit, content display skills, demand insight ability, product design and development ability.  </div>
          </div>
        </div>

        <div class="row">
          <div class="col-xs-3">
            <img src="./images/EIIR-Course.png" class="img-responsive" alt="Responsive image"><br><br> 
          </div>
          <div class="col-xs-8" align="justify">
            <div><strong>《具身智能机器人技术：算法与实践》(Embodied Intelligent Robotics: Algorithms and Practices)</strong></div>
            <div>Will come soon, plan to start from 2026 spring.</div>
            <div>It covers the basic introduciton of embodied intelligent robotics, world models, perceptions, task planning, and applications.</div>
          </div>
        </div>

      </div>
      <hr>

      <div id="students" class="container">
        <h3>Students (supervised or joint-supervised)</h3>  
        <h4>Current Ph.D. students</h4>
        <div>徐赵博、程曦、张超然</div> 

        <h4>Current master students</h4>
        <div><strong>Year 3 students:</strong> 雷睿祺、李晨睿、倪喆、黄定滔、林恩特、朱晓明、朴烽源、后胜、郑子杰</div>
        <div><strong>Year 2 students:</strong> <a href="https://scholar.google.com/citations?user=V6mBA5IAAAAJ&hl=en">张婉婷</a>、<a href="https://scholar.google.com/citations?user=9opMylYAAAAJ&hl=en">谢庆红冰</a>、黄炜航、<a href="https://scholar.google.com/citations?user=AXTFYCcAAAAJ&hl=en">李傅豪</a>、张超然、梁华岳、陈琰、李泽顺、杨永波、黄心柔、<a href="https://lzc-sg.github.io/">廖智超</a></div>
        <div><strong>Year 1 students:</strong> 周浩、汤彦哲、关智丞、李孟瑶、程航、李在勋、霍文言、阮心仪、王舒园</div>
        
        <h4>Have graduated students (including joint-supervised)</h4>
        <div><strong>Year 2025:</strong> 刘飞（工程博士）、倪喆、黄定滔、林恩特、朱晓明、朴烽源、后胜、<a href=" www.yuling.ai">郑子杰（驭灵科技公司）</a></div>
        <div><strong>Year 2024:</strong> 胡松、景星惠、黄家明、赖显松、谢宜含、<a href="https://crayon-shinchan.github.io/xinghui99.github.io/">李星辉</a>、孙萌、张浩、唐一凡、邰聪、陈昉星、陈一泓</div>
        <div><strong>Year 2023:</strong> <a href="https://mypage.just.edu.cn/jxgc/sz/list.htm">孙震（博士）</a>、王硕、张欣宇、杨凡、赵亮、熊鑫、王家政、郑政、李文湛、程渊术、苏俊宇</div>     
        <div><strong>Year 2022:</strong> 吕伟杰、史丰源、甘靖钊、刘冠宏、陶佳琪、欧雪燕、张少秋、章亦晨、肖磊才</div>   
        <div><strong>Year 2021:</strong> 付鑫、赵嘉宇、张鑫、俞佳熠、王宏羽、蒋昊雨</div>              
      </div>
    </div>
    <hr>
    <!-- <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?u=pwyk&d=XqwwDPMtd8r7hty0E7GpsRO6tUEvxWPWr4ygvS5gCd8"></script>-->
    <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&t=tt&d=lv35skyX2lbyweEWXclKdlDX6sBuXZH9CUyHouy4nk4'></script>

    <div class="mastfoot">
        <div class="inner">
          <p class="text-center">Updated April 2025, page created using <a href="http://getbootstrap.com/">Bootstrap</a></p>
        </div>
    </div>

    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script>window.jQuery || document.write('<script src="../../assets/js/vendor/jquery.min.js"><\/script>')</script>
    <script src="./bootstrap/dist/js/bootstrap.min.js"></script>
    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <script src="./bootstrap/assets/js/ie10-viewport-bug-workaround.js"></script>
  </body>
</html>
