<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="Long Zeng">
    <meta name="author" content="Long Zeng">
    <link rel="icon" href="./bootstrap/immv-logo.ico">

    <title>Dr. Zeng's homepage</title>

    <!-- Bootstrap core CSS, href gives URL of stylesheet; rel describes the relationship between this doc and the URL doc. -->
    <link href="./bootstrap/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <link href="./bootstrap/assets/css/ie10-viewport-bug-workaround.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="homepage.css" rel="stylesheet">

    <!-- Just for debugging purposes. Don't actually copy these 2 lines! -->
    <!--[if lt IE 9]><script src="../../assets/js/ie8-responsive-file-warning.js"></script><![endif]-->
    <script src="./bootstrap/assets/js/ie-emulation-modes-warning.js"></script>

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  </head>

  <body>

    <nav class="navbar navbar-inverse navbar-fixed-top">
      <div class="container">
        <div id="navbar" class="collapse navbar-collapse">
          <ul class="nav navbar-nav">
            <li><a href="#about">About Me</a></li>
            <li><a href="#news">News</a></li> 
            <li><a href="#pub">Publications</a></li>
            <li><a href="#teach">Teaching</a></li> 
            <li><a href="#projects">Projects</a></li>           
            <li><a href="#students">Students</a></li>
            <!--<li><a href="#pub">Services</a></li>  -->                  
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </nav>

    <div id="about" class="container">
        <h1>Long Zeng  曾龙</h1>
        <h4>Intelligent Manufacutring and Machine Vision Group</h4>
    </div>
    <hr>
    
    <div class="container">
      <div class="row">
        <div class="col-xs-2">
          <!--<img src="me.jpg" class="img-responsive" alt="Responsive image">-->
          <img src="zenglong.jpg" class="img-thumbnail" alt="Responsive image">
        </div>

        <div class="col-xs-3">
          <h4>Associate Professor, Ph.D.</h4>
          <address>
            <a href="https://www.sigs.tsinghua.edu.cn/">Tsinghua University</a>, <a href="https://www.sigs.tsinghua.edu.cn/">SIGS</a>, Mphil/Ph.D. Supervisor<br>
            <strong>--Director</strong>, Joint Research Center of Intelligent Service Robotic Technology;<br> 
            <strong>--Co-founder & Chief Scientist</strong>, <a href="http://www.gzfwzn.com/">Fuwei Intelligent Tech. Co. Ltd</a> (start-up);<br>
            <strong>--Technical advisor</strong>, <a href="https://www.pudutech.com/zh-CN">Pudu Tech. Co. Ltd</a> (Unicorn);
          </address><address>
            <strong>Email: <a href="zenglong@sz.tsinghua.edu.cn">zenglong@sz.tsinghua.edu.cn</a></strong><br>
            <strong><a href="https://www.sigs.tsinghua.edu.cn/cl/main.htm">[中文主页]</a>
              <a href="https://scholar.google.com/citations?user=72QbaQwAAAAJ&hl=en&oi=sra">[Google Scholar] </a>
              <a href="https://github.com/jackyzengl">[Github]</a></strong><br>
          </address>
        </div>

        <div class="col-xs-7" align="justify">
          <p>I am an tenure-tracked PI at the Information and Data Institute, SIGs, Tsinghua. I obtained my Ph.D. degree from Hong Kong University of Sci. and Tech..
            My research interest is Industrial Embodied Intelligence, to solve challenging problems in both design, manufacturing, and robotic delivery scenarios. 
            We study new deep learning methods exploring the unique characteristics of industrial data and mainly apply in <strong>Embodied Intelligent Industrial Robotics</strong> (EIIR, e.g.Flexible Assembly), 
            <strong>Embodied Intelligent Service Robotics</strong> (EISR), <strong>Intelligent CAD</strong> (e.g. Sketch-based Engineering Product Modeling). </p>
          <p>Our academic results are partially industrialized and verified by industrial products. I am the <strong>Co-founder & Chief Scientist</strong> 
            in <a href="http://www.gzfwzn.com/">Fuwei Intelligent Tech. Co. Ltd</a> (EIIR start-up company), the <strong>Technical advisor</strong>  in <a href="https://www.pudutech.com/zh-CN">Shenzhen Pudu Tech. Co. Ltd</a> (a <strong>Unicorn</strong> EISR company)
          </p> 
          <p>
            <strong>For Prospective Students</strong>: I am actively looking for highly self-motivated Ph.D., Master students, Post-doctor,
            and Research Assistants (undergraduate/graduate level). Please email me with your CV and transcripts if you are interested in our research.
          </p>          
        </div>
      </div>

      <hr>
      <div id="news" class="container">
        <h3>News</h3>
        <div><strong>[Jan.2025]</strong> Joined tenured-tracked PI, SIGS, Tsinghua University. <strong>New journey!</strong></div>
        <div><strong>[Oct.2024]</strong> Chair three session in IEEE IROS 2024 in Abu Dhabi!</strong></div>
        <div><strong>[Aug.2024]</strong> One paper was accpeted by Journal of Manufacturing System (<strong>IF 12.2</strong>)! Congratualations to Xu Zhaobo, Zhang chaoran, and Hu song.</div>               
        <div><strong>[Aug.2024]</strong> four papers were accpeted by IEEE IROS 2024! Congratualations to Ni zhe, Xie Yihan, Jing Xinghui, and Huang Dingtao</div>               
        <div><strong>[May.2024]</strong> Chair two session in IEEE ICRA 2024 in Yokohama Japan!</strong></div>
        <div><strong>[Nov.2014]</strong> Joined Tsinghua Shenzhen Graduate School, Tsinghua University.</div>
      </div>

      <hr>
      <h3 id="pub" class="container">
          <h3>Representative Publications</h3>
          <div> <a href="https://scholar.google.com/citations?user=72QbaQwAAAAJ&hl=en&oi=sra">Full paper list at Google Scholar</a>. <br>
          *,# indicates  corresponding and equal contribution author</div>
          <hr>
         <h3><strong>2025</strong></h3>

         <div class="row">
          <div class="col-xs-3">
            <img src="./images/2DGS-Room25IROS.png" class="img-responsive" alt="Responsive image"><br><br> 
          </div>
          <div class="col-xs-8" align="justify">  
            <div><strong>2DGS-Room: Seed-Guided 2D Gaussian Splatting with Geometric Constrains for High-Fidelity Indoor Scene Reconstruction</strong></div>
            <div>Wanting Zhang#, Haodong Xiang, Zhichao Liao, Xiansong Lai, Xinghui Li*, <strong>Long Zeng*</strong></strong>.</div>
            <div>Submitted to IEEE/RSJ IROS 2025.</div>
            <div><strong>Contribution:</strong>Recent advancements in 3D Gaussian Splatting have improved novel view synthesis with accelerated processing but have yet to deliver comparable performance in surface reconstruction. 
              In this paper, we introduce 2DGS-Room, a novel method leveraging 2D Gaussian Splatting for high-fidelity indoor scene reconstruction. Specifically, we employ a seed-guided mechanism to control the distribution of 2D Gaussians, 
              with the density of seed points dynamically optimized through adaptive growth and pruning mechanisms. To further improve geometric accuracy, we incorporate monocular depth and normal priors to provide constraints for details and textureless regions respectively. 
              Extensive experiments on ScanNet and ScanNet++ datasets demonstrate that our method achieves state-of-the-art performance in indoor scene reconstruction..</div>
            <div><a href="https://arxiv.org/abs/2412.03428">[ arXiv Paper ]</a>
                <a href="https://github.com/jackyzengl/2DGS-Room">[ Code ] </a>
            </div>
          </div>
        </div>

         <div class="row">
          <div class="col-xs-3">
            <img src="./images/GaussianRoom25ICRA.png" class="img-responsive" alt="Responsive image"><br><br> 
          </div>
          <div class="col-xs-8" align="justify">  
            <div><strong>GaussianRoom: Improving 3D Gaussian Splatting with SDF Guidance and Monocular Cues for Indoor Scene Reconstruction</strong></div>
            <div>Haodong Xiang#, Xinghui Li#, Kai Cheng#, Xiansong Lai, Wanting Zhang, Zhichao Liao, <strong>Long Zeng*</strong>, Xueping Liu*</strong>.</div>
            <div>IEEE Conference Robotic and Automation (<strong>ICRA</strong>), 2025.</div>
            <div><strong>Contribution:</strong> When 3D Gaussian Splatting(3DGS) is applied to indoor scenes with a significant number of textureless areas, 3DGS yields incomplete and noisy reconstruction results 
              due to the poor initialization of the point cloud and under-constrained optimization. Inspired by the continuity of signed distance field (SDF), which naturally has advantages in modeling surfaces, 
              we present a unified optimizing framework integrating neural SDF with 3DGS.This framework incorporates 3DGS, a learnable neural SDF field, and normal/edge priors. 
              Extensive experiments in ScanNet and ScanNet++ show that our method achieves state-of-the-art performance in both surface reconstruction and novel view synthesis.</div>
            <div><a href="https://arxiv.org/abs/2405.19671">[ arXiv Paper ]</a>
                <a href="https://github.com/jackyzengl/GaussianRoom">[ Code ] </a>
            </div>
          </div>
        </div>

         <h3><strong>2024</strong></h3>
          <div class="row">
            <div class="col-xs-3">
              <img src="./images/XuZB24JMS.png" class="img-responsive" alt="Responsive image"><br><br> 
            </div>
            <div class="col-xs-8" align="justify">  
              <div><strong>Reconfigurable Flexible Assembly Model and Its Implementation for Cross-Category Products</strong></div>
              <div>Zhaobo Xu#, Chaoran Zhang#, Song Hu#, Pingfa Feng, <strong>Long Zeng*</strong>.</div>
              <div>Journal of Manufacturing Systems (<strong>JMS, IF 12.2</strong>), 2024.</div>
              <div><strong>Contribution:</strong>As the production orders are becoming multi-category and small-batch in the era of product personalization, 
                these require frequent reconfiguration of reconfigurable flexible assembly system for cross-category products (RFAS-CCP). 
                However, there is no suitable theoretical assembly model and systematic implementation framework. We first propose a five-element assembly model (FAM) for RFAS-CCP, 
                i.e. product, process, resource, knowledge, and decision. We further reorganize various decision methods into a three-phase systematic implementation framework according to which stage they are used: 
                design, configuration, and operation phases. Finally, the effectiveness and practicality of the proposed five-element assembly model and three-phase systematic implementation framework are 
                experimented with a pressure reducing valve product.</div>
              <div><a href="https://www.sciencedirect.com/science/article/pii/S0278612524001857">[ Paper ]</a>
              </div>
            </div>
          </div>
        
          <div class="row">
            <div class="col-xs-3">
              <img src="./images/THUD24ICRA.png" class="img-responsive" alt="Responsive image"><br><br> 
            </div>
            <div class="col-xs-8" align="justify">
              <div><strong>Mobile Oriented Large-Scale Indoor Dataset for Dynamic Scene Understanding</strong></div>
              <div>Yi-Fan Tang#, Cong Tai#, Fang-Xin Chen#, Wanting Zhang, Tao Zhang, Yongjin Liu, <strong>Long Zeng*</strong>.</div>
              <div>IEEE Conference Robotic and Automation (<strong>ICRA</strong>), 2024.</div>
              <div><strong>Contribution:</strong> Most existing robotic datasets capture static scene data and thus are limited in evaluating robots dynamic performance. 
                To address this, we present a mobile robot oriented large-scale indoor dataset, denoted as THUD (Tsinghua University Dynamic) robotic dataset, for training and 
                evaluating their dynamic scene understanding algorithms. Our current dataset includes 13 larges-scale dynamic scenarios, 90K image frames, 20M 2D/3D bounding boxes of static and dynamic objects, 
                camera poses, and IMU. </div>
              <div><a href="https://arxiv.org/abs/2406.19791">[ Paper ]</a>&nbsp;         
              <a href="https://jackyzengl.github.io/THUD-Robotic-Dataset.github.io/">[ Project Page ]</a>&nbsp;
              </div>
            </div>
          </div>
        
          <div class="row">
            <div class="col-xs-3">
              <img src="./images/GRID24IROS.png" class="img-responsive" alt="Responsive image"><br><br> 
            </div>
            <div class="col-xs-8" align="justify">
              <div><strong>GRID:Scene-Graph-based Instruction-driven Robotic Task Planning</strong></div>
              <div>Zhe Ni#, Xiaoxin Deng#, Cong Tai#, Xinyue Zhu, Qinghongbing Xie, Weihang Huang, Xiang Wu, <strong>Long Zeng*</strong>.</div>
              <div>IEEE/RSJ International Conference on Intelligent Robots and Systems(<strong>IROS</strong>), 2024.</div>
              <div><strong>Contribution:</strong>Recent works have shown that Large Language Models (LLMs) can facilitate the grounding of instructions for robotic task planning. 
                Despite this progress, most existing works have primarily focused on utilizing raw images to aid LLMs in understanding environmental information. 
                However, this approach not only limits the scope of observation but also typically necessitates extensive multimodal data collection and large-scale models. 
                In this paper, we propose a novel approach called Graph-based Robotic Instruction Decomposer (GRID). Experiments have shown that our method outperforms GPT-4 by over 25.4% 
                in subtask accuracy and 43.6% in task accuracy. </div>
              <div><a href="https://arxiv.org/abs/2406.19791">[ Paper ]</a>&nbsp;         
              <a href="https://jackyzengl.github.io/GRID.github.io/">[ Project Page ]</a>&nbsp;
              <a href="https://github.com/jackyzengl/GRID">[ Code & Dataset ]</a>&nbsp;              
              </div>
            </div>
          </div>
          
          <h3><strong>Before 2024</strong></h3>


          <div class="row">
            <div class="col-xs-3">
              <img src="./images/pprnet++21TASE.png" class="img-responsive" alt="Responsive image"><br><br> 
            </div>
            <div class="col-xs-8" align="justify">
              <div><strong>PPR-Net++: Accurate 6D pose estimation in stacked scenarios</strong></div>
              <div><strong>Long Zeng#</strong>, W. J. Lv, Z. K. Dong, Y. J. Liu*.</div>
              <div>IEEE Transaction on Automation and Science (<strong>JCR Q1</strong>), 2021, pp.1-13.</div>
              <div><strong>Contribution:</strong>The challenge is that the learned network on the training dataset is no longer optimal on the testing dataset. 
                To address this problem, we propose a pose regression network PPR-Net++. It transforms each scene point into a point in the centroid space, followed by a clustering process and a voting
                process. When tested with the public synthetic Sileane dataset, our method is better in all eight objects, where five of them are improved by more than 5% in AP. On IPA real dataset, our method outperforms a large margin by 20%. This lays a solid foundation for robot grasping in industrial scenarios.</div>
              <div><a href="https://ieeexplore.ieee.org/document/9537584">[ Paper ]</a>&nbsp; 
              <a href="https://github.com/lvwj19/PPR-Net-plus"> [ Code & Dataset ] </a>&nbsp;              
              </div>
            </div>
          </div>

          <div class="row">
            <div class="col-xs-3">
              <img src="./images/pprnet19IROS.png" class="img-responsive" alt="Responsive image"><br><br> 
            </div>
            <div class="col-xs-8" align="justify">
              <div><strong>PPR-Net: Point-wise Pose Regression Network for Instance Segmentation and 6D Pose Estimation in Bin-picking Scenarios</strong></div>
              <div>Z. K. Dong#, S. C. Liu#, T. Zhou#, H. Cheng, <strong>Long Zeng*</strong>, X. Y. Yu, H. D. Liu.</div>
              <div>IEEE/RSJ International Conference on Intelligent Robots and Systems, Macau, China, pp. 1773-1780(<strong>IROS</strong>), 2019.</div>
              <div><strong>Contribution:</strong>This paper proposes a simple but novel Point-wise Pose Regression Network (PPR-Net). For each point in the
                point cloud, the network regresses a 6D pose of the object instance that the point belongs to. We argue that the regressed poses of points from the same object instance 
                should be located closely in pose space. PPR-Net outperforms the SOTA by 15% - 41% on benchmark Sileane dataset. </div>
              <div><a href="https://ieeexplore.ieee.org/document/8967895">[ Paper ]</a>&nbsp; 
              <a href="https://github.com/lvwj19/PPR-Net-plus"> [ Code & Dataset can be found in the paper PPR-Net++]</a>&nbsp;              
              </div>
            </div>
          </div>

          <div class="row">
            <div class="col-xs-3">
              <img src="./images/SpiderCNN18ECCV.png" class="img-responsive" alt="Responsive image"><br><br> 
            </div>
            <div class="col-xs-8" align="justify">
              <div><strong>SpiderCNN: Deep Learning on Point Sets with Parameterized Convolutional Filters</strong></div>
              <div>Yifan Xu# (My student), Tianqi Fan, Mengye Xu, <strong>Long Zeng</strong>, Yu Qiao*.</div>
              <div>European Conferenceon Computer Vision (<strong>ECCV</strong>), 2018.</div>
              <div><strong>Contribution:</strong>It is challenging to apply CNNs to domains lacking a regular underlying structures such as 3D point clouds. Thus, we propose a novel convolutional architecture, 
                termed SpiderCNN. A new paramerized convolutional operations is designed which can extend CNN from regular grids to irregular point sets. 
                SOTA performance on ModelNet40, i.e. 92:4%, was achieved (<strong>Google scholar citation >1000.</strong>).</div>
              <div><a href="https://link.springer.com/chapter/10.1007/978-3-030-01237-3_6">[ Paper ]</a>&nbsp; 
              <a href="https://github.com/xyf513/SpiderCNN">[ Code & Dataset ]</a>&nbsp;              
              </div>
            </div>
          </div>
      <hr>
      <div id="projects" class="container" align="justify">
        <h3>Projects</h3>
        <div>1. 2022/11-2025/10, Hand drawn sketch collection and generation algorithm, National Key Research and Development Program "Industrial Software" Key Special Project (Project No. 2022YFB3303101), PI.</div>
        <div>2. 2020/01-2023/12, National Natural Science Foundation of China (General Program # 61972220), Sketch-based parametric modeling with large data,<strong>PI</strong>.</div>
        <div>3. 2016/01-2018/12, National Natural Science Foundation of China (Youth Program #61502263), Sketch-based assembly modeling with Intelligent Feature, <strong>PI</strong>.</div>
        <div>4. 2022/01-2024/12, Research on 6D pose estimation method for parameterized parts in industrial stacking scenarios, Guangdong Natural Science General Project (No. 2022A1515011234), <strong>PI</strong>.</div>
        <div>5. 2021/11-2024/11, Tsinghua-Purdue Intelligent Service Robot Technology Joint Research Center, a college level institution, 11.0M, <strong>Director</strong>.</div>        
      </div>
      <hr>

      <div id="teach" class="container">
        <h3>Courses</h3>
        <div class="row">
          <div class="col-xs-3">
            <img src="./images/ML-Course.png" class="img-responsive" alt="Responsive image"><br><br> 
          </div>
          <div class="col-xs-8" align="justify">
            <div><strong>《机器学习实践与应用》(Machine Learning: Practices and Applications)</strong></div>
            <div>Lectured in every fall semester for graduate students, starting from 2021.</div>
            <div>It covers both machine learning methods (such as Linear Model, SVM, Bayesian classifier, Decision tree, and Cluster), 
              deep learning methods (such as AlexNet, ResNet et al.), and reinforcement learning. They are all organized by the same machine learning framework, 
              i.e. models, loss functions, and learning algorithms. </div>
          </div>
        </div>

        <div class="row">
          <div class="col-xs-3">
            <img src="./images/CAD-Course.png" class="img-responsive" alt="Responsive image"><br><br> 
          </div>
          <div class="col-xs-8" align="justify">
            <div><strong>《现代CAD方法与技术》(Modern CAD Methodology and Technology)</strong></div>
            <div>Lectured in every spring semester for graduate students, starting from 2017.</div>
            <div>It covers both traditional CAD methods (such as Bezier, B-spline curves and surface) and AI-based intelligent CAD methods.
              Based on the basic process of hand drawn engineering product modeling, this course is divided into several research topics. 
              In combination with the course project, the theory and classical algorithms of each topic are explained in simple terms, 
              including the pretreatment of hand drawn sketch, sketch scene understanding, basic theory of B-spline curve and surface, parametric model representation, 
              hand drawn model acquisition and other sketch modeling technologies, deep learning and its application in hand drawn modeling. </div>
          </div>
        </div>

        <div class="row">
          <div class="col-xs-3">
            <img src="./images/Product-Course.png" class="img-responsive" alt="Responsive image"><br><br> 
          </div>
          <div class="col-xs-8" align="justify">
            <div><strong>《产品设计与开发》(Product Design and Development)</strong></div>
            <div>Lectured in every fall semester for graduate students, starting from 2016.</div>
            <div>This course divides the whole process of product design and development into four stages: requirement analysis, conceptual design, detailed design and product exhibition. 
              The knowledge and methods used in each stage are organized and explained structurally, which is easy for students to master and apply. 
              The first three stages need to submit a product design progress report for each. In the stage of product exhibition, the course organizes a product design conference and 
              invites business people to act as judges. Excellent works have the opportunity to transform their achievements directly.
              The course emphasizes the organic combination of practice and theory. Students first form a project team and are equipped with project instructors. 
              Students apply the knowledge and methods learned in class directly to the curriculum project. In the process of project implementation, 
              students will exercise team spirit, content display skills, demand insight ability, product design and development ability.  </div>
          </div>
        </div>

        <div class="row">
          <div class="col-xs-3">
            <img src="./images/EIIR-Course.png" class="img-responsive" alt="Responsive image"><br><br> 
          </div>
          <div class="col-xs-8" align="justify">
            <div><strong>《具身智能机器人技术：算法与实践》(Embodied Intelligent Robotics: Algorithms and Practices)</strong></div>
            <div>Will come soon, plan to start from 2026 spring.</div>
            <div>It covers the basic introduciton of embodied intelligent robotics, world models, perceptions, task planning, and applications.</div>
          </div>
        </div>

      </div>
      <hr>

      <div id="students" class="container">
        <h3>Students (supervised or joint-supervised)</h3>  
        <h4>Current Ph.D. students</h4>
        <div>徐赵博、程曦</div>       
        <h4>Current master students</h4>
        <div><strong>Year 3 students:</strong> 雷睿祺、李晨睿、倪喆、黄定滔、林恩特、朱晓明、朴烽源、后胜、郑子杰</div>
        <div><strong>Year 2 students:</strong> 张婉婷、谢庆红冰、黄炜航、李傅豪、张超然、梁华岳、陈琰、李泽顺、杨永波、黄心柔、廖智超</div>
        <div><strong>Year 1 students:</strong> 周浩、汤彦哲、关智丞、李孟瑶、程航、李在勋、霍文言、阮心仪、王舒园</div>
      </div>
    </div>
    <hr>
    <!-- <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?u=pwyk&d=XqwwDPMtd8r7hty0E7GpsRO6tUEvxWPWr4ygvS5gCd8"></script>-->
    <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=400&t=n&d=lv35skyX2lbyweEWXclKdlDX6sBuXZH9CUyHouy4nk4&co=2d78ad&cmo=3acc3a&cmn=ff5353&ct=ffffff'></script>

    <div class="mastfoot">
        <div class="inner">
          <p class="text-center">Updated February 2022, page created using <a href="http://getbootstrap.com/">Bootstrap</a></p>
        </div>
    </div>

    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script>window.jQuery || document.write('<script src="../../assets/js/vendor/jquery.min.js"><\/script>')</script>
    <script src="./bootstrap/dist/js/bootstrap.min.js"></script>
    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <script src="./bootstrap/assets/js/ie10-viewport-bug-workaround.js"></script>
  </body>
</html>
